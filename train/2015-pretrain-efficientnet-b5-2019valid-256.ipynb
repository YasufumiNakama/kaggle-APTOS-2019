{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APTOS2019 train kernel (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flags for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "FOLD = 0\n",
    "MODEL = 'efficientnet-b5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'n_splits': 5,\n",
    "    'n_epochs': 12,\n",
    "    'lr': 1e-3,\n",
    "    'base_lr': 1e-4,\n",
    "    'max_lr': 3e-3,\n",
    "    'step_factor': 6,\n",
    "    'train_batch_size': 32,\n",
    "    'test_batch_size': 32,\n",
    "    'accumulation_steps': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/install.py:243: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\r\n",
      "  cmdoptions.check_install_build_global(options)\r\n",
      "Created temporary directory: /tmp/pip-ephem-wheel-cache-iskw613t\r\n",
      "Created temporary directory: /tmp/pip-req-tracker-sqh75l1i\r\n",
      "Created requirements tracker '/tmp/pip-req-tracker-sqh75l1i'\r\n",
      "Created temporary directory: /tmp/pip-install-tntyfdd3\r\n",
      "Processing /kaggle/input/nvidia-apex/repository/NVIDIA-apex-665b2dd\r\n",
      "  Created temporary directory: /tmp/pip-req-build-e66udlv4\r\n",
      "  Added file:///kaggle/input/nvidia-apex/repository/NVIDIA-apex-665b2dd to build tracker '/tmp/pip-req-tracker-sqh75l1i'\r\n",
      "    Running setup.py (path:/tmp/pip-req-build-e66udlv4/setup.py) egg_info for package from file:///kaggle/input/nvidia-apex/repository/NVIDIA-apex-665b2dd\r\n",
      "    Running command python setup.py egg_info\r\n",
      "    torch.__version__  =  1.1.0\r\n",
      "    running egg_info\r\n",
      "    creating pip-egg-info/apex.egg-info\r\n",
      "    writing pip-egg-info/apex.egg-info/PKG-INFO\r\n",
      "    writing dependency_links to pip-egg-info/apex.egg-info/dependency_links.txt\r\n",
      "    writing top-level names to pip-egg-info/apex.egg-info/top_level.txt\r\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\r\n",
      "    reading manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\r\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\r\n",
      "  Source in /tmp/pip-req-build-e66udlv4 has version 0.1, which satisfies requirement apex==0.1 from file:///kaggle/input/nvidia-apex/repository/NVIDIA-apex-665b2dd\r\n",
      "  Removed apex==0.1 from file:///kaggle/input/nvidia-apex/repository/NVIDIA-apex-665b2dd from build tracker '/tmp/pip-req-tracker-sqh75l1i'\r\n",
      "Skipping bdist_wheel for apex, due to binaries being disabled for it.\r\n",
      "Installing collected packages: apex\r\n",
      "  Created temporary directory: /tmp/pip-record-ml7nw0tq\r\n",
      "    Running command /opt/conda/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-e66udlv4/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-e66udlv4/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ml7nw0tq/install-record.txt --single-version-externally-managed --compile\r\n",
      "    torch.__version__  =  1.1.0\r\n",
      "\r\n",
      "    Compiling cuda extensions with\r\n",
      "    nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "    Copyright (c) 2005-2018 NVIDIA Corporation\r\n",
      "    Built on Sat_Aug_25_21:08:01_CDT_2018\r\n",
      "    Cuda compilation tools, release 10.0, V10.0.130\r\n",
      "    from /usr/local/cuda/bin\r\n",
      "\r\n",
      "    running install\r\n",
      "    running build\r\n",
      "    running build_py\r\n",
      "    creating build\r\n",
      "    creating build/lib.linux-x86_64-3.6\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex\r\n",
      "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\r\n",
      "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\r\n",
      "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/optimizers\r\n",
      "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\r\n",
      "    copying apex/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/optimizers\r\n",
      "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\r\n",
      "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\r\n",
      "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\r\n",
      "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/normalization\r\n",
      "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\r\n",
      "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    running build_ext\r\n",
      "    building 'apex_C' extension\r\n",
      "    creating build/temp.linux-x86_64-3.6\r\n",
      "    creating build/temp.linux-x86_64-3.6/csrc\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    building 'amp_C' extension\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    building 'fused_adam_cuda' extension\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/fused_adam_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/fused_adam_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    building 'syncbn' extension\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    building 'fused_layer_norm_cuda' extension\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    running install_lib\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fp16_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/normalization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/normalization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /opt/conda/lib/python3.6/site-packages/apex/normalization\r\n",
      "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    copying build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/cells.py to cells.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/models.py to models.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/compat.py to compat.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/opt.py to opt.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/amp.py to amp.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/utils.py to utils.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/handle.py to handle.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\r\n",
      "    running install_egg_info\r\n",
      "    running egg_info\r\n",
      "    creating apex.egg-info\r\n",
      "    writing apex.egg-info/PKG-INFO\r\n",
      "    writing dependency_links to apex.egg-info/dependency_links.txt\r\n",
      "    writing top-level names to apex.egg-info/top_level.txt\r\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\r\n",
      "    reading manifest file 'apex.egg-info/SOURCES.txt'\r\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\r\n",
      "    Copying apex.egg-info to /opt/conda/lib/python3.6/site-packages/apex-0.1-py3.6.egg-info\r\n",
      "    running install_scripts\r\n",
      "    writing list of installed files to '/tmp/pip-record-ml7nw0tq/install-record.txt'\r\n",
      "  Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Removing source in /tmp/pip-req-build-e66udlv4\r\n",
      "Successfully installed apex-0.1\r\n",
      "Cleaning up...\r\n",
      "Removed build tracker '/tmp/pip-req-tracker-sqh75l1i'\r\n",
      "1 location(s) to search for versions of pip:\r\n",
      "* https://pypi.org/simple/pip/\r\n",
      "Getting page https://pypi.org/simple/pip/\r\n",
      "Found index url https://pypi.org/simple\r\n",
      "Getting credentials from keyring for https://pypi.org/simple\r\n",
      "Getting credentials from keyring for pypi.org\r\n",
      "Starting new HTTPS connection (1): pypi.org:443\r\n",
      "Could not fetch URL https://pypi.org/simple/pip/: connection error: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7efc183a2e48>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)) - skipping\r\n",
      "Given no hashes to check 0 links for project 'pip': discarding no candidates\r\n"
     ]
    }
   ],
   "source": [
    "! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/nvidia-apex/repository/NVIDIA-apex-665b2dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/pretrained-models-pytorch/repository/Cadene-pretrained-models.pytorch-021d978')\n",
    "sys.path.append('../input/efficientnet-pytorch-repository/repository/lukemelas-EfficientNet-PyTorch-50a2bf2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from functools import partial\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, IAAAdditiveGaussianNoise, Normalize, OneOf,\n",
    "    RandomBrightness, RandomContrast, Resize, VerticalFlip, Rotate, ShiftScaleRotate,\n",
    "    RandomBrightnessContrast, OpticalDistortion, GridDistortion, ElasticTransform, Cutout\n",
    ")\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "from fastai.layers import Flatten, AdaptiveConcatPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file='train.log'):\n",
    "    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler\n",
    "    \n",
    "    log_format = '%(asctime)s %(levelname)s %(message)s'\n",
    "    \n",
    "    stream_handler = StreamHandler()\n",
    "    stream_handler.setLevel(DEBUG)\n",
    "    stream_handler.setFormatter(Formatter(log_format))\n",
    "    \n",
    "    file_handler = FileHandler(log_file)\n",
    "    file_handler.setFormatter(Formatter(log_format))\n",
    "    \n",
    "    logger = getLogger('APTOS')\n",
    "    logger.setLevel(DEBUG)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "LOG_FILE = 'aptos-train.log'\n",
    "LOGGER = init_logger(LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=777):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 777\n",
    "seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_hat, y):\n",
    "    return cohen_kappa_score(y_hat, y, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder():\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: official CyclicLR implementation doesn't work now\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CyclicLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, base_lr, max_lr, step_size, gamma=0.99, mode='triangular', last_epoch=-1):\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.mode = mode\n",
    "        assert mode in ['triangular', 'triangular2', 'exp_range']\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        new_lr = []\n",
    "        # make sure that the length of base_lrs doesn't change. Dont care about the actual value\n",
    "        for base_lr in self.base_lrs:\n",
    "            cycle = np.floor(1 + self.last_epoch / (2 * self.step_size))\n",
    "            x = np.abs(float(self.last_epoch) / self.step_size - 2 * cycle + 1)\n",
    "            if self.mode == 'triangular':\n",
    "                lr = self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x))\n",
    "            elif self.mode == 'triangular2':\n",
    "                lr = self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) / float(2 ** (cycle - 1))\n",
    "            elif self.mode == 'exp_range':\n",
    "                lr = self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * (self.gamma ** (self.last_epoch))\n",
    "            new_lr.append(lr)\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "APTOS_DIR = Path('../input/aptos2019-blindness-detection')\n",
    "APTOS_TRAIN_DIR = Path('../input/aptos-train-dataset')\n",
    "\n",
    "APTOS_TRAIN_IMAGES = APTOS_TRAIN_DIR / 'aptos-train-images/aptos-train-images'\n",
    "\n",
    "#APTOS_FOLDS = Path('../input/aptos-folds/folds.csv')\n",
    "#APTOS_FOLDS = Path('../input/aptos-folds/jpeg_folds.csv')\n",
    "#APTOS_TRAIN_FOLDS = Path('../input/aptos-folds/jpeg_folds_all.csv')\n",
    "#APTOS_VALID_FOLDS = Path('../input/aptos-folds/png_folds_all.csv')\n",
    "APTOS_TRAIN_FOLDS = Path('../input/aptos-folds/2015_5folds.csv')\n",
    "APTOS_VALID_FOLDS = Path('../input/aptos-folds/2019_5folds.csv')\n",
    "\n",
    "ID_COLUMN = 'id_code'\n",
    "TARGET_COLUMN = 'diagnosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_DIR = Path('../input/pytorch-pretrained-models')\n",
    "EFFICIENTNET_PRETRAINED_DIR = Path('../input/efficientnet-pytorch')\n",
    "\n",
    "PRETRAINED_MAPPING = {\n",
    "    # ResNet\n",
    "    'resnet18': PRETRAINED_DIR / 'resnet18-5c106cde.pth', \n",
    "    'resnet34': PRETRAINED_DIR / 'resnet34-333f7ec4.pth',\n",
    "    'resnet50': PRETRAINED_DIR / 'resnet50-19c8e357.pth',\n",
    "    'resnet101': PRETRAINED_DIR / 'resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': PRETRAINED_DIR / 'resnet152-b121ed2d.pth',\n",
    "\n",
    "    # ResNeXt\n",
    "    'resnext101_32x4d': PRETRAINED_DIR / 'resnext101_32x4d-29e315fa.pth',\n",
    "    'resnext101_64x4d': PRETRAINED_DIR / 'resnext101_64x4d-e77a0586.pth',\n",
    "\n",
    "    # WideResNet\n",
    "    #'wideresnet50'\n",
    "\n",
    "    # DenseNet\n",
    "    'densenet121': PRETRAINED_DIR / 'densenet121-fbdb23505.pth',\n",
    "    'densenet169': PRETRAINED_DIR / 'densenet169-f470b90a4.pth',\n",
    "    'densenet201': PRETRAINED_DIR / 'densenet201-5750cbb1e.pth',\n",
    "    'densenet161': PRETRAINED_DIR / 'densenet161-347e6b360.pth',\n",
    "\n",
    "    # SE-ResNet\n",
    "    'se_resnet50': PRETRAINED_DIR / 'se_resnet50-ce0d4300.pth',\n",
    "    'se_resnet101': PRETRAINED_DIR / 'se_resnet101-7e38fcc6.pth',\n",
    "    'se_resnet152': PRETRAINED_DIR / 'se_resnet152-d17c99b7.pth',\n",
    "\n",
    "    # SE-ResNeXt\n",
    "    'se_resnext50_32x4d': PRETRAINED_DIR / 'se_resnext50_32x4d-a260b3a4.pth',\n",
    "    'se_resnext101_32x4d': PRETRAINED_DIR / 'se_resnext101_32x4d-3b2fe3d8.pth',\n",
    "\n",
    "    # SE-Net\n",
    "    'senet154': PRETRAINED_DIR / 'senet154-c7b49a05.pth',\n",
    "\n",
    "    # InceptionV3\n",
    "    'inceptionv3': PRETRAINED_DIR / 'inception_v3_google-1a9a5a14.pth',\n",
    "\n",
    "    # InceptionV4\n",
    "    'inceptionv4': PRETRAINED_DIR / 'inceptionv4-8e4777a0.pth',\n",
    "\n",
    "    # BNInception\n",
    "    'bninception': PRETRAINED_DIR / 'bn_inception-52deb4733.pth',\n",
    "\n",
    "    # InceptionResNetV2\n",
    "    'inceptionresnetv2': PRETRAINED_DIR / 'inceptionresnetv2-520b38e4.pth',\n",
    "\n",
    "    # Xception\n",
    "    'xception': PRETRAINED_DIR / 'xception-43020ad28.pth',\n",
    "\n",
    "    # DualPathNet\n",
    "    'dpn68': PRETRAINED_DIR / 'dpn68-4af7d88d2.pth',\n",
    "    'dpn98': PRETRAINED_DIR / 'dpn98-722954780.pth',\n",
    "    'dpn131': PRETRAINED_DIR / 'dpn131-7af84be88.pth',\n",
    "    'dpn68b': PRETRAINED_DIR / 'dpn68b_extra-363ab9c19.pth',\n",
    "    'dpn92': PRETRAINED_DIR / 'dpn92_extra-fda993c95.pth',\n",
    "    'dpn107': PRETRAINED_DIR / 'dpn107_extra-b7f9f4cc9.pth',\n",
    "\n",
    "    # PolyNet\n",
    "    'polynet': PRETRAINED_DIR / 'polynet-f71d82a5.pth',\n",
    "\n",
    "    # NasNet-A-Large\n",
    "    'nasnetalarge': PRETRAINED_DIR / 'nasnetalarge-a1897284.pth',\n",
    "\n",
    "    # PNasNet-5-Large\n",
    "    'pnasnet5large': PRETRAINED_DIR / 'pnasnet5large-bf079911.pth',\n",
    "\n",
    "    # EfficientNet\n",
    "    'efficientnet-b0': EFFICIENTNET_PRETRAINED_DIR / 'efficientnet-b0-08094119.pth',\n",
    "    'efficientnet-b1': EFFICIENTNET_PRETRAINED_DIR / 'efficientnet-b1-dbc7070a.pth',\n",
    "    'efficientnet-b2': EFFICIENTNET_PRETRAINED_DIR / 'efficientnet-b2-27687264.pth',\n",
    "    'efficientnet-b3': EFFICIENTNET_PRETRAINED_DIR / 'efficientnet-b3-c8376fa2.pth',\n",
    "    'efficientnet-b4': EFFICIENTNET_PRETRAINED_DIR / 'efficientnet-b4-e116e8b3.pth',\n",
    "    'efficientnet-b5': EFFICIENTNET_PRETRAINED_DIR / 'efficientnet-b5-586e6cc6.pth',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APTOSTrainDataset(Dataset):\n",
    "    def __init__(self, image_dir, file_paths, labels, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = f'{self.image_dir}/{self.file_paths[idx]}'\n",
    "        label = torch.tensor(self.labels[idx]).float()\n",
    "        \n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "def crop_image_from_gray(img, tol=7):\n",
    "    \"\"\"\n",
    "    Crop out black borders\n",
    "    https://www.kaggle.com/ratthachat/aptos-updated-preprocessing-ben-s-cropping\n",
    "    \"\"\"  \n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0):\n",
    "            return img\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img\n",
    "\n",
    "\n",
    "class CircleCrop(ImageOnlyTransform):\n",
    "    def __init__(self, tol=7, always_apply=False, p=1.0):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.tol = tol\n",
    "    \n",
    "    def apply(self, img, **params):\n",
    "        img = crop_image_from_gray(img)    \n",
    "    \n",
    "        height, width, depth = img.shape    \n",
    "    \n",
    "        x = int(width/2)\n",
    "        y = int(height/2)\n",
    "        r = np.amin((x,y))\n",
    "    \n",
    "        circle_img = np.zeros((height, width), np.uint8)\n",
    "        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n",
    "        img = cv2.bitwise_and(img, img, mask=circle_img)\n",
    "        img = crop_image_from_gray(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        return img \n",
    "    \n",
    "\n",
    "class CircleCropV2(ImageOnlyTransform):\n",
    "    def __init__(self, tol=7, always_apply=False, p=1.0):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.tol = tol\n",
    "    \n",
    "    def apply(self, img, **params):\n",
    "        img = crop_image_from_gray(img)\n",
    "        \n",
    "        height, width, depth = img.shape\n",
    "        largest_side = np.max((height, width))\n",
    "        img = cv2.resize(img, (largest_side, largest_side))\n",
    "    \n",
    "        height, width, depth = img.shape    \n",
    "    \n",
    "        x = int(width/2)\n",
    "        y = int(height/2)\n",
    "        r = np.amin((x,y))\n",
    "    \n",
    "        circle_img = np.zeros((height, width), np.uint8)\n",
    "        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n",
    "        img = cv2.bitwise_and(img, img, mask=circle_img)\n",
    "        img = crop_image_from_gray(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(*, data):\n",
    "    assert data in ('train', 'valid')\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            CircleCropV2(),\n",
    "            Resize(256, 256),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            Rotate(p=0.5), \n",
    "            #ShiftScaleRotate(p=0.5),\n",
    "            #RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, p=0.5),\n",
    "            #OpticalDistortion(distort_limit=(0.9,1.0), shift_limit=0.05, interpolation=1, border_mode=4, \n",
    "            #                  value=None, always_apply=False, p=0.5),\n",
    "            #GridDistortion(num_steps=5, distort_limit=0.3, interpolation=1, border_mode=4,\n",
    "            #               value=None, always_apply=False, p=0.5),\n",
    "            #ElasticTransform(alpha=1, sigma=50, alpha_affine=50, interpolation=1, border_mode=4,\n",
    "            #                 value=None, always_apply=True, approximate=False, p=0.5),\n",
    "            Cutout(p=0.25, max_h_size=25, max_w_size=25, num_holes=8),\n",
    "            #OneOf([\n",
    "            #    RandomBrightness(0.1, p=1),\n",
    "            #    RandomContrast(0.1, p=1),\n",
    "            #], p=0.25),\n",
    "            RandomContrast(0.5, p=0.5),\n",
    "            IAAAdditiveGaussianNoise(p=0.25),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            CircleCropV2(),\n",
    "            Resize(256, 256),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensor(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModule(nn.Sequential):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__(\n",
    "            nn.BatchNorm1d(n_features),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(n_features, n_features),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(n_features),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_features, 1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, model_name='resnet50', weights_path=None):\n",
    "        assert model_name in ('resnet50', 'resnet101', 'resnet152')\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = pretrainedmodels.__dict__[model_name](pretrained=None)\n",
    "        self.net.load_state_dict(torch.load(weights_path))\n",
    "        \n",
    "        n_features = self.net.last_linear.in_features\n",
    "        \n",
    "        self.net.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        # self.net.avgpool = AdaptiveConcatPool2d(1)\n",
    "        self.net.last_linear = ClassifierModule(n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNeXt(nn.Module):\n",
    "    def __init__(self, model_name='resnext101_32x4d', weights_path=None):\n",
    "        assert model_name in ('resnext101_32x4d', 'resnext101_64x4d')\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = pretrainedmodels.__dict__[model_name](pretrained=None)\n",
    "        self.net.load_state_dict(torch.load(weights_path))\n",
    "        \n",
    "        n_features = self.net.last_linear.in_features\n",
    "        \n",
    "        self.net.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # self.net.avg_pool = AdaptiveConcatPool2d(1)\n",
    "        self.net.last_linear = ClassifierModule(n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSENet(nn.Module):\n",
    "    def __init__(self, model_name='se_resnet50', weights_path=None):\n",
    "        assert model_name in ('senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d')\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = pretrainedmodels.__dict__[model_name](pretrained=None)\n",
    "        self.net.load_state_dict(torch.load(weights_path))\n",
    "        \n",
    "        n_features = self.net.last_linear.in_features\n",
    "        \n",
    "        self.net.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # self.net.avg_pool = AdaptiveConcatPool2d(1)\n",
    "        self.net.last_linear = ClassifierModule(n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet-b0', weights_path=None):\n",
    "        assert model_name in ('efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5')\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = EfficientNet.from_name(model_name)\n",
    "        self.net.load_state_dict(torch.load(weights_path))\n",
    "        \n",
    "        n_features = self.net._fc.in_features\n",
    "        \n",
    "        self.net._fc = ClassifierModule(n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-13 20:29:22,451 DEBUG Fold: 0\n",
      "2019-08-13 20:29:22,452 DEBUG Model: efficientnet-b5\n",
      "2019-08-13 20:29:22,454 DEBUG Train params: {'n_splits': 5, 'n_epochs': 12, 'lr': 0.001, 'base_lr': 0.0001, 'max_lr': 0.003, 'step_factor': 6, 'train_batch_size': 32, 'test_batch_size': 32, 'accumulation_steps': 10}\n"
     ]
    }
   ],
   "source": [
    "LOGGER.debug(f'Fold: {FOLD}')\n",
    "LOGGER.debug(f'Model: {MODEL}')\n",
    "LOGGER.debug(f'Train params: {train_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-13 20:29:22,470 INFO [Prepare train and valid sets] start\n",
      "2019-08-13 20:29:22,471 INFO [  * load folds csv] start\n",
      "2019-08-13 20:29:22,522 INFO [  * load folds csv] done in 0 s.\n",
      "2019-08-13 20:29:22,523 INFO [  * define dataset] start\n",
      "2019-08-13 20:29:22,525 INFO [  * define dataset] done in 0 s.\n",
      "2019-08-13 20:29:22,525 INFO [  * define dataloader] start\n",
      "2019-08-13 20:29:22,526 INFO [  * define dataloader] done in 0 s.\n",
      "2019-08-13 20:29:22,527 INFO [Prepare train and valid sets] done in 0 s.\n",
      "2019-08-13 20:29:22,528 DEBUG train size: 28099, valid size: 3534\n"
     ]
    }
   ],
   "source": [
    "with timer('Prepare train and valid sets'):\n",
    "    with timer('  * load folds csv'):\n",
    "        #folds = pd.read_csv(APTOS_FOLDS)\n",
    "        #train_fold = folds[folds['fold'] != FOLD].reset_index(drop=True)\n",
    "        #valid_fold = folds[folds['fold'] == FOLD].reset_index(drop=True)\n",
    "        folds = pd.read_csv(APTOS_TRAIN_FOLDS)\n",
    "        train_fold = folds[folds['fold'] != FOLD].reset_index(drop=True)\n",
    "        #valid_fold2015 = folds[folds['fold'] == FOLD].reset_index(drop=True)\n",
    "        #valid_fold2019 = pd.read_csv(APTOS_VALID_FOLDS)\n",
    "        #valid_fold = pd.concat([valid_fold2015, valid_fold2019]).reset_index(drop=True)\n",
    "        valid_fold = pd.read_csv(APTOS_VALID_FOLDS)\n",
    "    \n",
    "    with timer('  * define dataset'):\n",
    "        APTOSTrainDataset = partial(APTOSTrainDataset, image_dir=APTOS_TRAIN_IMAGES)\n",
    "        train_dataset = APTOSTrainDataset(file_paths=train_fold.id_code.values,\n",
    "                                          labels=train_fold.diagnosis.values[:, np.newaxis],\n",
    "                                          transform=get_transforms(data='train'))\n",
    "        valid_dataset = APTOSTrainDataset(file_paths=valid_fold.id_code.values,\n",
    "                                          labels=valid_fold.diagnosis.values[:, np.newaxis],\n",
    "                                          transform=get_transforms(data='valid'))\n",
    "        \n",
    "    with timer('  * define dataloader'):\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=train_params['train_batch_size'],\n",
    "                                  shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=train_params['test_batch_size'],\n",
    "                                  shuffle=False)\n",
    "        \n",
    "LOGGER.debug(f'train size: {len(train_dataset)}, valid size: {len(valid_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-13 20:29:22,556 INFO [Train model] start\n",
      "2019-08-13 21:15:08,407 DEBUG   Epoch 1 - avg_train_loss: 0.1044  avg_val_loss: 0.5345  time: 2740s\n",
      "2019-08-13 21:15:08,410 DEBUG           - qwk: 0.837237  coefficients: [0.593729 0.964637 2.150397 4.286211]\n",
      "2019-08-13 21:59:22,356 DEBUG   Epoch 2 - avg_train_loss: 0.0632  avg_val_loss: 0.4578  time: 2654s\n",
      "2019-08-13 21:59:22,357 DEBUG           - qwk: 0.852277  coefficients: [0.573924 1.478256 2.123378 3.624947]\n",
      "2019-08-13 22:43:14,399 DEBUG   Epoch 3 - avg_train_loss: 0.0569  avg_val_loss: 0.4669  time: 2632s\n",
      "2019-08-13 22:43:14,401 DEBUG           - qwk: 0.876678  coefficients: [0.488998 1.423123 2.826116 3.701562]\n",
      "2019-08-13 23:26:27,984 DEBUG   Epoch 4 - avg_train_loss: 0.0558  avg_val_loss: 1.2862  time: 2593s\n",
      "2019-08-13 23:26:27,986 DEBUG           - qwk: 0.865539  coefficients: [0.371486 1.19794  3.909248 4.590181]\n",
      "2019-08-14 00:09:34,049 DEBUG   Epoch 5 - avg_train_loss: 0.0553  avg_val_loss: 0.5706  time: 2586s\n",
      "2019-08-14 00:09:34,050 DEBUG           - qwk: 0.846752  coefficients: [0.538831 1.497804 2.731017 3.280475]\n",
      "2019-08-14 00:53:35,204 DEBUG   Epoch 6 - avg_train_loss: 0.0559  avg_val_loss: 0.5437  time: 2641s\n",
      "2019-08-14 00:53:35,206 DEBUG           - qwk: 0.883198  coefficients: [0.520216 0.932087 3.093656 4.119455]\n",
      "2019-08-14 01:37:57,652 DEBUG   Epoch 7 - avg_train_loss: 0.0558  avg_val_loss: 0.5602  time: 2662s\n",
      "2019-08-14 01:37:57,654 DEBUG           - qwk: 0.878223  coefficients: [0.684408 1.379475 2.543153 3.336112]\n",
      "2019-08-14 02:22:37,595 DEBUG   Epoch 8 - avg_train_loss: 0.0519  avg_val_loss: 0.5824  time: 2680s\n",
      "2019-08-14 02:22:37,597 DEBUG           - qwk: 0.844334  coefficients: [0.511636 1.379088 2.815893 3.549785]\n",
      "2019-08-14 03:07:28,577 DEBUG   Epoch 9 - avg_train_loss: 0.0477  avg_val_loss: 0.5010  time: 2691s\n",
      "2019-08-14 03:07:28,578 DEBUG           - qwk: 0.892101  coefficients: [0.479506 1.485265 2.839248 3.843124]\n",
      "2019-08-14 03:52:37,192 DEBUG   Epoch 10 - avg_train_loss: 0.0432  avg_val_loss: 0.5476  time: 2708s\n",
      "2019-08-14 03:52:37,193 DEBUG           - qwk: 0.894184  coefficients: [0.467361 1.592342 2.852242 3.678763]\n",
      "2019-08-14 04:38:03,158 DEBUG   Epoch 11 - avg_train_loss: 0.0406  avg_val_loss: 0.3344  time: 2726s\n",
      "2019-08-14 04:38:03,161 DEBUG           - qwk: 0.898806  coefficients: [0.50021  1.607307 2.581715 3.146798]\n",
      "2019-08-14 05:23:28,034 DEBUG   Epoch 12 - avg_train_loss: 0.0378  avg_val_loss: 0.3779  time: 2725s\n",
      "2019-08-14 05:23:28,035 DEBUG           - qwk: 0.889866  coefficients: [0.523498 1.619272 2.653837 3.454206]\n",
      "2019-08-14 05:23:28,225 INFO [Train model] done in 32046 s.\n"
     ]
    }
   ],
   "source": [
    "with timer('Train model'):\n",
    "    n_epochs = train_params['n_epochs']\n",
    "    lr = train_params['lr']\n",
    "    base_lr = train_params['base_lr']\n",
    "    max_lr = train_params['max_lr']\n",
    "    step_factor = train_params['step_factor']\n",
    "    test_batch_size = train_params['test_batch_size']\n",
    "    accumulation_steps = train_params['accumulation_steps']\n",
    "    \n",
    "    model = CustomEfficientNet(model_name=MODEL, weights_path=PRETRAINED_MAPPING[MODEL])\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=lr, amsgrad=False)\n",
    "    #optimizer = SGD(model.parameters(), lr=lr, weight_decay=4e-5, momentum=0.9, nesterov=True)\n",
    "    scheduler = CyclicLR(optimizer,\n",
    "                         base_lr=base_lr,\n",
    "                         max_lr=max_lr,\n",
    "                         step_size=len(train_loader) * step_factor)\n",
    "\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    optimized_rounder = OptimizedRounder()\n",
    "    y_true = valid_fold.diagnosis.values\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            if isinstance(scheduler, CyclicLR):\n",
    "                scheduler.step()\n",
    "\n",
    "            y_preds = model(images.to(device))\n",
    "            loss = criterion(y_preds, labels.to(device))\n",
    "\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "\n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            avg_loss += loss.item() / accumulation_steps / len(train_loader)\n",
    "\n",
    "        if not isinstance(scheduler, CyclicLR):\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_preds = np.zeros((len(valid_dataset)))\n",
    "        avg_val_loss = 0.\n",
    "\n",
    "        for i, (images, labels) in enumerate(valid_loader):\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(images.to(device)).detach()\n",
    "\n",
    "            loss = criterion(y_preds, labels.to(device))\n",
    "            valid_preds[i * test_batch_size: (i+1) * test_batch_size] = y_preds[:, 0].to('cpu').numpy()\n",
    "\n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "\n",
    "        optimized_rounder.fit(valid_preds, y_true)\n",
    "        coefficients = optimized_rounder.coefficients()\n",
    "        final_preds = optimized_rounder.predict(valid_preds, coefficients)\n",
    "        qwk = quadratic_weighted_kappa(y_true, final_preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.debug(f'          - qwk: {qwk:.6f}  coefficients: {coefficients}')\n",
    "\n",
    "        # FIXME: save all epochs for debug\n",
    "        torch.save(model.state_dict(), f'{MODEL}_fold{FOLD}_epoch{epoch+1}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
